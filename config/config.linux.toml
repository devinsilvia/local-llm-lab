# Perplexica configuration - Linux profile
# This file assumes native Ollama on Linux with GPU acceleration when available.

[general]
# Directory that Perplexica can scan for local documents.
documents_dir = "/app/documents"

[models]
# Default model name; replace with your preferred Ollama model.
# Use a tool-capable model so web search works out of the box.
default = "llama3.1:8b-instruct-q4_0"

[models.providers.ollama]
# Linux profile settings (native Ollama only)
BASE_URL = "http://host.docker.internal:11434"

# Linux guidance:
# - Integrated Intel GPUs are not well supported; treat as CPU-only.
# - NVIDIA GPUs (CUDA) are best supported; AMD support varies with ROCm.
# Example alternatives: "llama3:8b-instruct-q4_0", "llama3:14b-instruct-q4_0".

[search]
# Tunable settings for retrieval behavior.
results_limit = 8

[retriever]
# Adjust concurrency based on your GPU and system load.
max_concurrency = 4
