# Perplexica configuration - Windows profile
# This file assumes native Ollama on Windows with GPU acceleration when available.

[general]
# Directory that Perplexica can scan for local documents.
documents_dir = "/app/documents"

[models]
# Default model name; replace with your preferred Ollama model.
default = "llama3"

[models.providers.ollama]
# Windows profile settings (native Ollama only)
BASE_URL = "http://host.docker.internal:11434"

# Windows guidance: 8-14B models with quantization are reasonable on midrange GPUs.
# Example alternatives: "llama3:8b-instruct-q4_0", "llama3:14b-instruct-q4_0".

[search]
# Tunable settings for retrieval behavior.
results_limit = 8

[retriever]
# Adjust concurrency based on your GPU and system load.
max_concurrency = 4
